skip_under_ci
----

# This test can now roughly equalize both cpu and write bandwidth. It didn't
# use to be able to do this, because the highest cpu node had the lowest write
# bandwidth and vice versa, so neither was able to shed to the other. The
# ignoreLevel logic in rebalanceStores with the grace duration to start
# shedding more aggressively and other related changes have made this much
# better.

gen_cluster nodes=6 node_cpu_rate_capacity=50000
----
WARNING: node CPU capacity of â‰ˆ0.00 cores is likely accidental

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes=268435456
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=500000 min_key=1 max_key=10000
----

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=36 min_key=10001 max_key=20000 placement_type=replica_placement bytes=268435456
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

eval duration=60m samples=1 seed=42 cfgs=(mma-only,mma-and-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases) full=true
----
cpu#1: last:  [s1=83373599, s2=82955301, s3=83154536, s4=83182126, s5=83144874, s6=84444013] (stddev=492899.15, mean=83375741.50, sum=500254449)
cpu#1: thrash_pct: [s1=27%, s2=39%, s3=33%, s4=16%, s5=17%, s6=15%]  (sum=147%)
cpu_util#1: last:  [s1=1667.47, s2=1659.11, s3=1663.09, s4=1663.64, s5=1662.90, s6=1688.88] (stddev=9.86, mean=1667.51, sum=10005)
cpu_util#1: thrash_pct: [s1=27%, s2=39%, s3=33%, s4=16%, s5=17%, s6=15%]  (sum=147%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=6, s2=14, s3=15, s4=25, s5=6, s6=6] (stddev=6.95, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=16%, s2=21%, s3=14%, s4=38%, s5=0%, s6=0%]  (sum=89%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=35, s2=54, s3=53, s4=25, s5=25, s6=24] (stddev=12.91, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=81%, s2=8%, s3=15%, s4=38%, s5=91%, s6=44%]  (sum=277%)
write_bytes_per_second#1: last:  [s1=7810763, s2=10583352, s3=10582678, s4=10533730, s5=10534887, s6=9975971] (stddev=1003663.69, mean=10003563.50, sum=60021381)
write_bytes_per_second#1: thrash_pct: [s1=1%, s2=6%, s3=7%, s4=3%, s5=156%, s6=116%]  (sum=289%)
artifacts[mma-only]: d7b075e1bfa8343f
==========================
cpu#1: last:  [s1=82965298, s2=84522849, s3=83421549, s4=83231678, s5=82980418, s6=83177529] (stddev=532667.09, mean=83383220.17, sum=500299321)
cpu#1: thrash_pct: [s1=91%, s2=94%, s3=94%, s4=116%, s5=54%, s6=40%]  (sum=487%)
cpu_util#1: last:  [s1=1659.31, s2=1690.46, s3=1668.43, s4=1664.63, s5=1659.61, s6=1663.55] (stddev=10.65, mean=1667.66, sum=10006)
cpu_util#1: thrash_pct: [s1=91%, s2=94%, s3=94%, s4=116%, s5=54%, s6=40%]  (sum=487%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=9, s2=14, s3=12, s4=16, s5=10, s6=11] (stddev=2.38, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=101%, s2=80%, s3=91%, s4=152%, s5=84%, s6=90%]  (sum=598%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=35, s2=36, s3=36, s4=35, s5=38, s6=36] (stddev=1.00, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=635%, s2=500%, s3=567%, s4=802%, s5=537%, s6=517%]  (sum=3557%)
write_bytes_per_second#1: last:  [s1=9476647, s2=10526461, s3=9974506, s4=8919911, s5=10533058, s6=10588441] (stddev=625653.67, mean=10003170.67, sum=60019024)
write_bytes_per_second#1: thrash_pct: [s1=145%, s2=129%, s3=135%, s4=217%, s5=257%, s6=228%]  (sum=1111%)
artifacts[mma-and-count]: d3ae4c23b9872f21
==========================
Cluster Set Up
	n1(AU_EAST,AU_EAST_1,0vcpu): {s1:(256GiB)}
	n2(AU_EAST,AU_EAST_1,0vcpu): {s2:(256GiB)}
	n3(AU_EAST,AU_EAST_1,0vcpu): {s3:(256GiB)}
	n4(AU_EAST,AU_EAST_1,0vcpu): {s4:(256GiB)}
	n5(AU_EAST,AU_EAST_1,0vcpu): {s5:(256GiB)}
	n6(AU_EAST,AU_EAST_1,0vcpu): {s6:(256GiB)}
Key Space
	[1,10000): 36(rf=3), 256MiB, [{s1*,s2,s3}:36]
	[10001,20000): 36(rf=3), 256MiB, [{s4*,s5,s6}:36]
Event
	set LBRebalancingMode to 3
Workload Set Up
	[1,10000): read-only high-cpu [500.00cpu-us/op, 1B/op, 1000ops/s]
	[10001,20000): write-only large-block [0.00cpu-us/write(raft), 1000B/op, 20000ops/s]
Changed Settings
	SplitQueueEnabled: false (default: true)
==========================
