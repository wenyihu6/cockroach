skip_under_ci
----

# This test can now roughly equalize both cpu and write bandwidth. It didn't
# use to be able to do this, because the highest cpu node had the lowest write
# bandwidth and vice versa, so neither was able to shed to the other. The
# ignoreLevel logic in rebalanceStores with the grace duration to start
# shedding more aggressively and other related changes have made this much
# better.

gen_cluster nodes=6 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes=268435456
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=500000 min_key=1 max_key=10000
----

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=36 min_key=10001 max_key=20000 placement_type=replica_placement bytes=268435456
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

eval duration=60m samples=1 seed=42 cfgs=(mma-only,mma-and-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases) full=true
----
cpu#1: last:  [s1=83246697, s2=97418757, s3=70456984, s4=41417957, s5=138483264, s6=69246369] (stddev=29891612.75, mean=83378338.00, sum=500270028)
cpu#1: thrash_pct: [s1=25%, s2=46%, s3=79%, s4=24%, s5=20%, s6=27%]  (sum=221%)
cpu_util#1: last:  [s1=0.02, s2=0.02, s3=0.01, s4=0.01, s5=0.03, s6=0.01] (stddev=0.01, mean=0.02, sum=0)
cpu_util#1: thrash_pct: [s1=25%, s2=46%, s3=79%, s4=24%, s5=20%, s6=27%]  (sum=221%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=12, s2=13, s3=16, s4=8, s5=14, s6=9] (stddev=2.77, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=188%, s2=154%, s3=167%, s4=189%, s5=166%, s6=158%]  (sum=1024%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=35, s2=35, s3=37, s4=37, s5=38, s6=34] (stddev=1.41, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=287%, s2=271%, s3=379%, s4=379%, s5=293%, s6=228%]  (sum=1837%)
write_bytes_per_second#1: last:  [s1=10029881, s2=9974320, s3=9423738, s4=10584991, s5=10583210, s6=9419594] (stddev=474834.79, mean=10002622.33, sum=60015734)
write_bytes_per_second#1: thrash_pct: [s1=397%, s2=308%, s3=407%, s4=736%, s5=724%, s6=688%]  (sum=3259%)
artifacts[mma-only]: adfd732847aae677
==========================
cpu#1: last:  [s1=138473107, s2=42945459, s3=110896995, s4=55420101, s5=83226005, s6=69295629] (stddev=32679087.72, mean=83376216.00, sum=500257296)
cpu#1: thrash_pct: [s1=57%, s2=70%, s3=69%, s4=35%, s5=31%, s6=37%]  (sum=299%)
cpu_util#1: last:  [s1=0.03, s2=0.01, s3=0.02, s4=0.01, s5=0.02, s6=0.01] (stddev=0.01, mean=0.02, sum=0)
cpu_util#1: thrash_pct: [s1=57%, s2=70%, s3=69%, s4=35%, s5=31%, s6=37%]  (sum=299%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=14, s2=12, s3=17, s4=12, s5=9, s6=8] (stddev=3.00, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=170%, s2=165%, s3=219%, s4=171%, s5=198%, s6=187%]  (sum=1110%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=35, s2=37, s3=38, s4=35, s5=35, s6=36] (stddev=1.15, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=379%, s2=316%, s3=375%, s4=406%, s5=332%, s6=347%]  (sum=2156%)
write_bytes_per_second#1: last:  [s1=9425596, s2=10581698, s3=9977867, s4=9471716, s5=10032082, s6=10528475] (stddev=452428.78, mean=10002905.67, sum=60017434)
write_bytes_per_second#1: thrash_pct: [s1=453%, s2=391%, s3=476%, s4=703%, s5=743%, s6=718%]  (sum=3484%)
artifacts[mma-and-count]: 9ec393f24cf055c1
==========================
Cluster Set Up
	n1(AU_EAST,AU_EAST_1,5vcpu): {s1:(256GiB)}
	n2(AU_EAST,AU_EAST_1,5vcpu): {s2:(256GiB)}
	n3(AU_EAST,AU_EAST_1,5vcpu): {s3:(256GiB)}
	n4(AU_EAST,AU_EAST_1,5vcpu): {s4:(256GiB)}
	n5(AU_EAST,AU_EAST_1,5vcpu): {s5:(256GiB)}
	n6(AU_EAST,AU_EAST_1,5vcpu): {s6:(256GiB)}
Key Space
	[1,10000): 36(rf=3), 256MiB, [{s1*,s2,s3}:36]
	[10001,20000): 36(rf=3), 256MiB, [{s4*,s5,s6}:36]
Event
	set LBRebalancingMode to 4
Workload Set Up
	[1,10000): read-only high-cpu [500.00cpu-us/op, 1B/op, 1000ops/s]
	[10001,20000): write-only large-block [0.00cpu-us/write(raft), 1000B/op, 20000ops/s]
Changed Settings
	SplitQueueEnabled: false (default: true)
==========================
