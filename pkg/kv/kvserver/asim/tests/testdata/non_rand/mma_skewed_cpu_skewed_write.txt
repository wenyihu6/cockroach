skip_under_ci
----

# This test can now roughly equalize both cpu and write bandwidth. It didn't
# use to be able to do this, because the highest cpu node had the lowest write
# bandwidth and vice versa, so neither was able to shed to the other. The
# ignoreLevel logic in rebalanceStores with the grace duration to start
# shedding more aggressively and other related changes have made this much
# better.

gen_cluster nodes=6 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes=268435456
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=500000 min_key=1 max_key=10000
----

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=36 min_key=10001 max_key=20000 placement_type=replica_placement bytes=268435456
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

eval duration=60m samples=1 seed=42 cfgs=(mma-only,mma-and-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases) full=true
----
cpu#1: last:  [s1=110766292, s2=69191937, s3=41661042, s4=56707323, s5=97068724, s6=124798205] (stddev=29737913.23, mean=83365587.17, sum=500193523)
cpu#1: thrash_pct: [s1=33%, s2=73%, s3=56%, s4=20%, s5=32%, s6=21%]  (sum=235%)
cpu_util#1: last:  [s1=0.02, s2=0.01, s3=0.01, s4=0.01, s5=0.02, s6=0.02] (stddev=0.01, mean=0.02, sum=0)
cpu_util#1: thrash_pct: [s1=33%, s2=73%, s3=56%, s4=20%, s5=32%, s6=21%]  (sum=235%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=14, s2=16, s3=13, s4=8, s5=9, s6=12] (stddev=2.77, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=106%, s2=104%, s3=74%, s4=54%, s5=90%, s6=96%]  (sum=523%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=35, s2=38, s3=35, s4=34, s5=38, s6=36] (stddev=1.53, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=509%, s2=603%, s3=371%, s4=465%, s5=465%, s6=338%]  (sum=2751%)
write_bytes_per_second#1: last:  [s1=9479199, s2=10534716, s3=10025552, s4=8922786, s5=10527418, s6=10526940] (stddev=615628.04, mean=10002768.50, sum=60016611)
write_bytes_per_second#1: thrash_pct: [s1=148%, s2=148%, s3=77%, s4=335%, s5=340%, s6=340%]  (sum=1388%)
artifacts[mma-only]: b8cc05b7581b6357
==========================
cpu#1: last:  [s1=152532487, s2=83208038, s3=56765543, s4=55367664, s5=69208816, s6=83162717] (stddev=32853172.82, mean=83374210.83, sum=500245265)
cpu#1: thrash_pct: [s1=24%, s2=51%, s3=50%, s4=19%, s5=13%, s6=16%]  (sum=174%)
cpu_util#1: last:  [s1=0.03, s2=0.02, s3=0.01, s4=0.01, s5=0.01, s6=0.02] (stddev=0.01, mean=0.02, sum=0)
cpu_util#1: thrash_pct: [s1=24%, s2=51%, s3=50%, s4=19%, s5=13%, s6=16%]  (sum=174%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=17, s2=15, s3=15, s4=7, s5=9, s6=9] (stddev=3.79, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=93%, s2=68%, s3=68%, s4=54%, s5=90%, s6=90%]  (sum=463%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=37, s2=37, s3=36, s4=34, s5=36, s6=36] (stddev=1.00, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=422%, s2=382%, s3=420%, s4=464%, s5=400%, s6=360%]  (sum=2448%)
write_bytes_per_second#1: last:  [s1=8868599, s2=10028646, s3=10030050, s4=9975112, s5=10585005, s6=10527899] (stddev=563217.00, mean=10002551.83, sum=60015311)
write_bytes_per_second#1: thrash_pct: [s1=113%, s2=39%, s3=88%, s4=280%, s5=257%, s6=279%]  (sum=1057%)
artifacts[mma-and-count]: 6946549ba6e5f514
==========================
Cluster Set Up
	n1(AU_EAST,AU_EAST_1,5vcpu): {s1:(256GiB)}
	n2(AU_EAST,AU_EAST_1,5vcpu): {s2:(256GiB)}
	n3(AU_EAST,AU_EAST_1,5vcpu): {s3:(256GiB)}
	n4(AU_EAST,AU_EAST_1,5vcpu): {s4:(256GiB)}
	n5(AU_EAST,AU_EAST_1,5vcpu): {s5:(256GiB)}
	n6(AU_EAST,AU_EAST_1,5vcpu): {s6:(256GiB)}
Key Space
	[1,10000): 36(rf=3), 256MiB, [{s1*,s2,s3}:36]
	[10001,20000): 36(rf=3), 256MiB, [{s4*,s5,s6}:36]
Event
	set LBRebalancingMode to 4
Workload Set Up
	[1,10000): read-only high-cpu [500.00cpu-us/op, 1B/op, 1000ops/s]
	[10001,20000): write-only large-block [0.00cpu-us/write(raft), 1000B/op, 20000ops/s]
Changed Settings
	SplitQueueEnabled: false (default: true)
==========================
