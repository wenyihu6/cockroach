skip_under_ci
----

# This test can now roughly equalize both cpu and write bandwidth. It didn't
# use to be able to do this, because the highest cpu node had the lowest write
# bandwidth and vice versa, so neither was able to shed to the other. The
# ignoreLevel logic in rebalanceStores with the grace duration to start
# shedding more aggressively and other related changes have made this much
# better.

gen_cluster nodes=6 node_cpu_rate_capacity=50000
----
WARNING: node CPU capacity of â‰ˆ0.00 cores is likely accidental

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes=268435456
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=500000 min_key=1 max_key=10000
----

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=36 min_key=10001 max_key=20000 placement_type=replica_placement bytes=268435456
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

eval duration=60m samples=1 seed=42 cfgs=(mma-only,mma-and-count) metrics=(cpu,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=84404667, s2=83342482, s3=83284638, s4=83223773, s5=83120410, s6=82928924] (stddev=475325.17, mean=83384149.00, sum=500304894)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=10, s2=11, s3=13, s4=16, s5=12, s6=10] (stddev=2.08, mean=12.00, sum=72)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=35, s2=36, s3=36, s4=36, s5=36, s6=37] (stddev=0.58, mean=36.00, sum=216)
write_bytes_per_second#1: last:  [s1=8920924, s2=10032104, s3=10032073, s4=10530618, s5=10529804, s6=9972666] (stddev=536874.30, mean=10003031.50, sum=60018189)
artifacts[mma-only]: 69d5ea73505d556b
cpu#1: last:  [s1=83248327, s2=83210584, s3=83163956, s4=84314390, s5=83287653, s6=83089142] (stddev=420065.93, mean=83385675.33, sum=500314052)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=15, s2=10, s3=15, s4=12, s5=10, s6=10] (stddev=2.24, mean=12.00, sum=72)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=36, s2=36, s3=33, s4=38, s5=37, s6=36] (stddev=1.53, mean=36.00, sum=216)
write_bytes_per_second#1: last:  [s1=8869835, s2=9976536, s3=9422900, s4=10582759, s5=10582017, s6=10584517] (stddev=662170.98, mean=10003094.00, sum=60018564)
artifacts[mma-and-count]: 7f610ee196e8be23
