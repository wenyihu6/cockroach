# This test verifies that the allocator correctly handles disk fullness by
#  shedding replicas from stores that exceed the storage capacity threshold. It
# sets up a cluster where one store (s5) has significantly less capacity (100 GiB)
# than others (512 GiB), causing it to hit the disk fullness threshold and
# continuously shed replicas to maintain disk usage less than 95%.
skip_under_ci
----

# Set every store's capacity to 512 GiB, we will later adjust just one store to
# have less free capacity.
gen_cluster nodes=5 store_byte_capacity_gib=512
----

gen_ranges ranges=500 bytes_mib=286
----

gen_load rate=500 max_block=60000 min_block=60000
----
29 MiB/s goodput

# Set the disk storage capacity of s5 to 100 GiB. This will necessitate
# shedding replicas from s5 continously as the workload fills up ranges.
set_capacity store=5 capacity=107374182400
----

# We will repeatedly hit the disk fullness threshold which causes shedding
# replicas on store 5. We should see s5 hovering right around 92.5-95%
# (the storage capacity threshold value).
eval duration=20m seed=42 metrics=(replicas,disk_fraction_used) cfgs=(sma-count,mma-only,mma-count)
----
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.24, s2=0.24, s3=0.24, s4=0.24, s5=0.93] (stddev=0.28, mean=0.38, sum=2)
disk_fraction_used#1: thrash_pct: [s1=8%, s2=10%, s3=10%, s4=10%, s5=27%]  (sum=65%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=318, s2=313, s3=314, s4=316, s5=239] (stddev=30.55, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=113%, s2=145%, s3=140%, s4=140%, s5=0%]  (sum=538%)
artifacts[sma-count]: e195d75759d9f951
==========================
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.25, s2=0.25, s3=0.25, s4=0.23, s5=0.84] (stddev=0.24, mean=0.36, sum=2)
disk_fraction_used#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=27%]  (sum=27%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=328, s2=329, s3=327, s4=300, s5=216] (stddev=43.38, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
artifacts[mma-only]: d17cd609d7e897ff
==========================
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.24, s2=0.24, s3=0.25, s4=0.25, s5=0.84] (stddev=0.24, mean=0.36, sum=2)
disk_fraction_used#1: thrash_pct: [s1=10%, s2=11%, s3=11%, s4=13%, s5=26%]  (sum=71%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=318, s2=318, s3=325, s4=323, s5=216] (stddev=42.09, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=104%, s2=109%, s3=114%, s4=140%, s5=0%]  (sum=467%)
artifacts[mma-count]: 271a24eb8e524953
==========================
