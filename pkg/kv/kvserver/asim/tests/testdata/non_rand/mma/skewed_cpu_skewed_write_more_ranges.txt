# This test is similar to skewed_cpu_skewed_write.txt, but with more ranges 300. 
# To have the same amount of total load, bytes per range is reduced to 26 MiB.
# 
# Expected outcome: The allocator should rebalance both cpu and write load across
# all stores, with mma achieving better results than sma. 
gen_cluster nodes=6 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 26 MiB.
gen_ranges ranges=300 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=26
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=5000000 min_key=1 max_key=10000
----
5.00 access-vcpus

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=300 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=26
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----
0.00 raft-vcpus, 19 MiB/s goodput

setting split_queue_enabled=false
----

eval duration=75m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=2986028863, s2=753578625, s3=724902110, s4=199120110, s5=199098998, s6=115815333] (stddev=997618092.12, mean=829757339.83, sum=4978544039)
cpu#1: thrash_pct: [s1=7%, s2=29%, s3=29%, s4=13%, s5=12%, s6=8%]  (sum=98%)
cpu_util#1: last:  [s1=0.60, s2=0.15, s3=0.14, s4=0.04, s5=0.04, s6=0.02] (stddev=0.20, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=7%, s2=29%, s3=29%, s4=13%, s5=12%, s6=8%]  (sum=98%)
leases#1: first: [s1=300, s2=0, s3=0, s4=300, s5=0, s6=0] (stddev=141.42, mean=100.00, sum=600)
leases#1: last:  [s1=183, s2=121, s3=99, s4=178, s5=12, s6=7] (stddev=70.48, mean=100.00, sum=600)
leases#1: thrash_pct: [s1=1%, s2=0%, s3=0%, s4=11%, s5=0%, s6=0%]  (sum=12%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300, s6=300] (stddev=0.00, mean=300.00, sum=1800)
replicas#1: last:  [s1=290, s2=456, s3=456, s4=178, s5=178, s6=242] (stddev=116.84, mean=300.00, sum=1800)
replicas#1: thrash_pct: [s1=15%, s2=0%, s3=0%, s4=11%, s5=10%, s6=6%]  (sum=43%)
write_bytes_per_second#1: last:  [s1=1591017, s2=10496292, s3=10496258, s4=10950972, s5=10951761, s6=15496755] (stddev=4147851.60, mean=9997175.83, sum=59983055)
write_bytes_per_second#1: thrash_pct: [s1=0%, s2=11%, s3=12%, s4=2%, s5=100%, s6=98%]  (sum=223%)
artifacts[mma-only]: d21afd3186116e3c
==========================
cpu#1: last:  [s1=872576964, s2=771784222, s3=808058936, s4=903578426, s5=808066123, s6=832592270] (stddev=43853244.42, mean=832776156.83, sum=4996656941)
cpu#1: thrash_pct: [s1=14%, s2=63%, s3=66%, s4=41%, s5=31%, s6=30%]  (sum=246%)
cpu_util#1: last:  [s1=0.17, s2=0.15, s3=0.16, s4=0.18, s5=0.16, s6=0.17] (stddev=0.01, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=14%, s2=63%, s3=66%, s4=41%, s5=31%, s6=30%]  (sum=246%)
leases#1: first: [s1=300, s2=0, s3=0, s4=300, s5=0, s6=0] (stddev=141.42, mean=100.00, sum=600)
leases#1: last:  [s1=106, s2=112, s3=113, s4=92, s5=95, s6=82] (stddev=11.27, mean=100.00, sum=600)
leases#1: thrash_pct: [s1=62%, s2=63%, s3=59%, s4=61%, s5=66%, s6=66%]  (sum=377%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300, s6=300] (stddev=0.00, mean=300.00, sum=1800)
replicas#1: last:  [s1=334, s2=341, s3=336, s4=276, s5=257, s6=256] (stddev=37.63, mean=300.00, sum=1800)
replicas#1: thrash_pct: [s1=236%, s2=285%, s3=275%, s4=272%, s5=233%, s6=190%]  (sum=1491%)
write_bytes_per_second#1: last:  [s1=7267921, s2=9529082, s3=9139020, s4=10808825, s5=10919009, s6=11734586] (stddev=1464841.19, mean=9899740.50, sum=59398443)
write_bytes_per_second#1: thrash_pct: [s1=65%, s2=61%, s3=59%, s4=144%, s5=169%, s6=153%]  (sum=651%)
artifacts[mma-count]: afbb4699d1fecf0a
==========================
