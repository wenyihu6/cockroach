# This test verifies the allocator's behavior with opposing CPU and write load
# distributions. The test sets up a 6-node cluster where s1-s3 handle high-CPU
# read traffic and s4-s6 handle high-write traffic. Initially, all leases are on
# s1 and s4.
#
# Expected outcome: The allocator should rebalance both cpu and write load across
# all stores, with mma achieving better results than sma.
#
# This test can now roughly equalize both cpu and write bandwidth. It didn't
# use to be able to do this, because the highest cpu node had the lowest write
# bandwidth and vice versa, so neither was able to shed to the other. The
# ignoreLevel logic in rebalanceStores with the grace duration to start
# shedding more aggressively and other related changes have made this much
# better.
gen_cluster nodes=6 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=256
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=5000000 min_key=1 max_key=10000
----
5.00 access-vcpus

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=36 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=256
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----
0.00 raft-vcpus, 19 MiB/s goodput

setting split_queue_enabled=false
----

eval duration=38m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases) full=true
----
cpu#1: last:  [s1=1237762200, s2=827471171, s3=964924708, s4=696956830, s5=566073102, s6=689488767] (stddev=220446965.86, mean=830446129.67, sum=4982676778)
cpu#1: thrash_pct: [s1=8%, s2=35%, s3=44%, s4=14%, s5=14%, s6=13%]  (sum=128%)
cpu_util#1: last:  [s1=0.25, s2=0.17, s3=0.19, s4=0.14, s5=0.11, s6=0.14] (stddev=0.04, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=8%, s2=35%, s3=44%, s4=14%, s5=14%, s6=13%]  (sum=128%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=9, s2=14, s3=16, s4=24, s5=4, s6=5] (stddev=6.90, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=0%, s2=14%, s3=21%, s4=33%, s5=0%, s6=0%]  (sum=68%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=41, s2=53, s3=52, s4=24, s5=23, s6=23] (stddev=13.24, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=51%, s2=20%, s3=20%, s4=31%, s5=31%, s6=31%]  (sum=183%)
write_bytes_per_second#1: last:  [s1=7814741, s2=10581827, s3=10581393, s4=10521008, s5=10520183, s6=9962735] (stddev=999605.84, mean=9996981.17, sum=59981887)
write_bytes_per_second#1: thrash_pct: [s1=2%, s2=14%, s3=8%, s4=2%, s5=116%, s6=111%]  (sum=252%)
artifacts[mma-only]: 56e29d211ddc7319
==========================
cpu#1: last:  [s1=695627595, s2=691060088, s3=835357055, s4=821288221, s5=958616102, s6=971229206] (stddev=111014165.63, mean=828863044.50, sum=4973178267)
cpu#1: thrash_pct: [s1=39%, s2=99%, s3=83%, s4=62%, s5=45%, s6=27%]  (sum=355%)
cpu_util#1: last:  [s1=0.14, s2=0.14, s3=0.17, s4=0.16, s5=0.19, s6=0.19] (stddev=0.02, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=39%, s2=99%, s3=83%, s4=62%, s5=45%, s6=27%]  (sum=355%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=14, s2=12, s3=13, s4=13, s5=11, s6=9] (stddev=1.63, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=82%, s2=79%, s3=68%, s4=83%, s5=73%, s6=66%]  (sum=452%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=35, s2=38, s3=35, s4=35, s5=36, s6=37] (stddev=1.15, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=459%, s2=460%, s3=501%, s4=444%, s5=329%, s6=330%]  (sum=2523%)
write_bytes_per_second#1: last:  [s1=10028097, s2=10036519, s3=9475998, s4=10519917, s5=9416880, s6=10523151] (stddev=439835.33, mean=10000093.67, sum=60000562)
write_bytes_per_second#1: thrash_pct: [s1=68%, s2=62%, s3=103%, s4=209%, s5=170%, s6=198%]  (sum=811%)
artifacts[mma-count]: 8cb72521f4d8d8ef
==========================
Cluster Set Up
	n1(AU_EAST,AU_EAST_1,5vcpu): {s1:(256GiB)}
	n2(AU_EAST,AU_EAST_1,5vcpu): {s2:(256GiB)}
	n3(AU_EAST,AU_EAST_1,5vcpu): {s3:(256GiB)}
	n4(AU_EAST,AU_EAST_1,5vcpu): {s4:(256GiB)}
	n5(AU_EAST,AU_EAST_1,5vcpu): {s5:(256GiB)}
	n6(AU_EAST,AU_EAST_1,5vcpu): {s6:(256GiB)}
Key Space
	[1,10000): 36(rf=3), 256MiB, [{s1*,s2,s3}:36]
	[10001,20000): 36(rf=3), 256MiB, [{s4*,s5,s6}:36]
Event
	set LBRebalancingMode to 3
Workload Set Up
	[1,10000): read-only high-cpu [5000.00cpu-us/op, 1B/op, 1000ops/s]
	[10001,20000): write-only large-block [0.00cpu-us/write(raft), 1000B/op, 20000ops/s]
Changed Settings
	SplitQueueEnabled: false (default: true)
==========================
