# This test verifies that for remotely cpu-overloaded stores, mma wait for lease
# shedding grace period (remoteStoreLeaseSheddingGraceDuration) before rebalancing
# replicas away from the store. The test sets up a 5-node cluster where store s1
# has a high replica count (25 out of 75 total replicas) but holds no leases. All
# leases are distributed among stores s2-s5. A write-only workload with high raft
# CPU cost creates CPU pressure primarily on s1 due to its replica count, but s1
# cannot shed leases since it holds none.
#
# Expected outcome:
# Want to test two cases:
# (1) high_cpu_unable_to_shed_leases.txt: Where its impossible to shed leases
# from the cpu overloaded s1, so we should initially observe a period of no
# rebalancing activity away from the store before
# any replica based rebalancing.
# (2) high_cpu_able_to_shed_leases.txt: Where its possible to shed leases from
# the CPU overloaded s1, so we should observe a period of lease transfers before
# any replica based rebalancing away from the store occurs.

gen_cluster nodes=5 node_cpu_rate_capacity=9000000000
----

setting split_queue_enabled=false
----

gen_ranges ranges=25 min_key=0 max_key=10000 placement_type=replica_placement
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6
----
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6

gen_load rate=5000 rw_ratio=0 min_key=0 max_key=10000 raft_cpu_per_write=1000000
----
5.00 raft-vcpus, 4.9 KiB/s goodput

eval duration=5m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=5000000000, s2=2596866666, s3=2598100000, s4=2401900000, s5=2403133333] (stddev=1003794154.51, mean=2999999999.80, sum=14999999999)
cpu#1: thrash_pct: [s1=0%, s2=7%, s3=8%, s4=8%, s5=7%]  (sum=28%)
cpu_util#1: last:  [s1=0.56, s2=0.29, s3=0.29, s4=0.27, s5=0.27] (stddev=0.11, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=0%, s2=7%, s3=8%, s4=8%, s5=7%]  (sum=28%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=4999, s2=2596, s3=2598, s4=2401, s5=2403] (stddev=1003.60, mean=2999.40, sum=14997)
write_bytes_per_second#1: thrash_pct: [s1=8%, s2=7%, s3=8%, s4=8%, s5=7%]  (sum=37%)
artifacts[mma-only]: f3cff51960bf30dd
==========================
cpu#1: last:  [s1=3002333333, s2=2994900000, s3=3195446666, s4=3001113333, s5=2806206666] (stddev=123116164.26, mean=2999999999.60, sum=14999999998)
cpu#1: thrash_pct: [s1=23%, s2=26%, s3=22%, s4=23%, s5=22%]  (sum=116%)
cpu_util#1: last:  [s1=0.33, s2=0.33, s3=0.36, s4=0.33, s5=0.31] (stddev=0.01, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=23%, s2=26%, s3=22%, s4=23%, s5=22%]  (sum=116%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=15, s2=15, s3=16, s4=15, s5=14] (stddev=0.63, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=3002, s2=2994, s3=3195, s4=3001, s5=2806] (stddev=123.05, mean=2999.60, sum=14998)
write_bytes_per_second#1: thrash_pct: [s1=22%, s2=26%, s3=21%, s4=22%, s5=21%]  (sum=112%)
artifacts[mma-count]: 4ab68b4fbf079583
==========================
