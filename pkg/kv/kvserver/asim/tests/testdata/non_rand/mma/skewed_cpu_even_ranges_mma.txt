# The test sets up a 9-node cluster where every store initiallyhas the same
# number of replicas.
# 
# s1 handles significant cpu load, s2,s3 handle small follower cpu load, while
# s4-s9 handle minimal cpu load and high write bandwidth.
#
# Expected outcome: The allocator should rebalance both leases and replicas to
# achieve more even cpu and write distribution across all nodes.
gen_cluster nodes=9 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=256
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

# 5ms of request CPU per access and 500Âµs of raft CPU per write @ 1000/s.
gen_load rate=1000 rw_ratio=0.95 min_block=100 max_block=100 request_cpu_per_access=5000000 raft_cpu_per_write=500000 min_key=1 max_key=10000
----

# Almost empty workload, which generates no CPU and small amount of writes
# over the second half of the keyspace, scattered over s4-s9.
gen_ranges ranges=72 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=256
{s4,s5,s6}:1
{s7,s8,s9}:1
----
{s4:*,s5,s6}:1
{s7:*,s8,s9}:1

gen_load rate=100 rw_ratio=0 min_block=128 max_block=128 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

# TODO(tbg): it's interesting that sma-only does better on write throughput than
# mma-only. Looking at the graphs, the mma-only flavor is much slower in moving
# load around. Possibly a bug?
eval duration=22m samples=1 seed=42 cfgs=(mma-count) metrics=(cpu,cpu_util,leases,replicas,write_bytes_per_second)
----
cpu#1: last:  [s1=573230151, s2=569529908, s3=572459710, s4=558992526, s5=557598402, s6=553755878, s7=553049302, s8=559268212, s9=571593326] (stddev=7842750.59, mean=563275268.33, sum=5069477415)
cpu#1: thrash_pct: [s1=73%, s2=114%, s3=105%, s4=19%, s5=49%, s6=11%, s7=41%, s8=75%, s9=35%]  (sum=521%)
cpu_util#1: last:  [s1=0.11, s2=0.11, s3=0.11, s4=0.11, s5=0.11, s6=0.11, s7=0.11, s8=0.11, s9=0.11] (stddev=0.00, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=73%, s2=114%, s3=105%, s4=19%, s5=49%, s6=11%, s7=41%, s8=75%, s9=35%]  (sum=521%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=7, s2=7, s3=7, s4=15, s5=14, s6=14, s7=15, s8=14, s9=15] (stddev=3.56, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=66%, s2=89%, s3=77%, s4=34%, s5=39%, s6=8%, s7=52%, s8=62%, s9=21%]  (sum=448%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=36, s2=35, s3=35, s4=36, s5=38, s6=35, s7=38, s8=35, s9=36] (stddev=1.15, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=257%, s2=317%, s3=317%, s4=143%, s5=177%, s6=117%, s7=119%, s8=288%, s9=171%]  (sum=1906%)
write_bytes_per_second#1: last:  [s1=5300, s2=5305, s3=5230, s4=6145, s5=6520, s6=6028, s7=6527, s8=6079, s9=6215] (stddev=487.77, mean=5927.67, sum=53349)
write_bytes_per_second#1: thrash_pct: [s1=1113%, s2=961%, s3=1091%, s4=436%, s5=553%, s6=458%, s7=579%, s8=715%, s9=592%]  (sum=6498%)
artifacts[mma-count]: 17484343e2d663dc
==========================
