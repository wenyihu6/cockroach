# The test sets up a 9-node cluster where every store initiallyhas the same
# number of replicas.
# 
# s1 handles significant cpu load, s2,s3 handle small follower cpu load, while
# s4-s9 handle minimal cpu load and high write bandwidth.
#
# Expected outcome: The allocator should rebalance both leases and replicas to
# achieve more even cpu and write distribution across all nodes.
gen_cluster nodes=9 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=256
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

# 5ms of request CPU per access and 500Âµs of raft CPU per write @ 1000/s.
gen_load rate=1000 rw_ratio=0.95 min_block=100 max_block=100 request_cpu_per_access=5000000 raft_cpu_per_write=500000 min_key=1 max_key=10000
----
5.00 access-vcpus, 0.03 raft-vcpus, 4.9 KiB/s goodput

# Almost empty workload, which generates no CPU and small amount of writes
# over the second half of the keyspace, scattered over s4-s9.
gen_ranges ranges=72 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=256
{s4,s5,s6}:1
{s7,s8,s9}:1
----
{s4:*,s5,s6}:1
{s7:*,s8,s9}:1

# TODO(tbg): this is barely anything, is this intentional?
gen_load rate=100 rw_ratio=0 min_block=128 max_block=128 min_key=10001 max_key=20000
----
12 KiB/s goodput

setting split_queue_enabled=false
----

# TODO(tbg): it's interesting that sma-only does better on write throughput than
# mma-only. Looking at the graphs, the mma-only flavor is much slower in moving
# load around. Possibly a bug?
eval duration=22m samples=1 seed=42 cfgs=(sma-count,mma-only,mma-count) metrics=(cpu,cpu_util,leases,replicas,write_bytes_per_second)
----
cpu#1: last:  [s1=716247676, s2=845205426, s3=708752854, s4=282679345, s5=695628387, s6=564043353, s7=281135855, s8=3661083, s9=984527888] (stddev=296368439.47, mean=564653540.78, sum=5081881867)
cpu#1: thrash_pct: [s1=47%, s2=59%, s3=42%, s4=7%, s5=63%, s6=14%, s7=8%, s8=11%, s9=50%]  (sum=301%)
cpu_util#1: last:  [s1=0.14, s2=0.17, s3=0.14, s4=0.06, s5=0.14, s6=0.11, s7=0.06, s8=0.00, s9=0.20] (stddev=0.06, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=47%, s2=59%, s3=42%, s4=7%, s5=63%, s6=14%, s7=8%, s8=11%, s9=50%]  (sum=301%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=11, s2=11, s3=9, s4=15, s5=13, s6=11, s7=16, s8=9, s9=13] (stddev=2.31, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=95%, s2=61%, s3=26%, s4=28%, s5=114%, s6=32%, s7=52%, s8=32%, s9=85%]  (sum=525%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=37, s2=36, s3=37, s4=36, s5=34, s6=37, s7=36, s8=35, s9=36] (stddev=0.94, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=939%, s2=512%, s3=189%, s4=188%, s5=1065%, s6=276%, s7=288%, s8=176%, s9=888%]  (sum=4520%)
write_bytes_per_second#1: last:  [s1=6169, s2=5895, s3=5829, s4=6129, s5=5672, s6=6281, s7=6299, s8=6122, s9=5901] (stddev=204.48, mean=6033.00, sum=54297)
write_bytes_per_second#1: thrash_pct: [s1=1274%, s2=985%, s3=817%, s4=888%, s5=1525%, s6=976%, s7=907%, s8=915%, s9=1388%]  (sum=9674%)
artifacts[sma-count]: 845b681abae743bb
==========================
cpu#1: last:  [s1=1258106813, s2=724430038, s3=731467778, s4=291045116, s5=555884789, s6=140880308, s7=558907398, s8=410381029, s9=418506451] (stddev=304401149.05, mean=565512191.11, sum=5089609720)
cpu#1: thrash_pct: [s1=9%, s2=52%, s3=58%, s4=5%, s5=9%, s6=3%, s7=19%, s8=5%, s9=10%]  (sum=172%)
cpu_util#1: last:  [s1=0.25, s2=0.14, s3=0.15, s4=0.06, s5=0.11, s6=0.03, s7=0.11, s8=0.08, s9=0.08] (stddev=0.06, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=9%, s2=52%, s3=58%, s4=5%, s5=9%, s6=3%, s7=19%, s8=5%, s9=10%]  (sum=172%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=9, s2=5, s3=5, s4=38, s5=4, s6=1, s7=40, s8=3, s9=3] (stddev=14.58, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=0%, s2=27%, s3=32%, s4=0%, s5=0%, s6=0%, s7=6%, s8=0%, s9=0%]  (sum=64%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=30, s2=31, s3=30, s4=38, s5=40, s6=37, s7=40, s8=39, s9=39] (stddev=4.11, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%, s7=22%, s8=0%, s9=0%]  (sum=22%)
write_bytes_per_second#1: last:  [s1=4336, s2=4474, s3=4297, s4=6676, s5=6977, s6=6529, s7=7000, s8=6886, s9=6861] (stddev=1165.01, mean=6004.00, sum=54036)
write_bytes_per_second#1: thrash_pct: [s1=564%, s2=572%, s3=538%, s4=168%, s5=201%, s6=153%, s7=228%, s8=185%, s9=203%]  (sum=2813%)
artifacts[mma-only]: fc7165e88d11bf49
==========================
cpu#1: last:  [s1=1126298732, s2=985065667, s3=986038000, s4=281828953, s5=419152763, s6=137273458, s7=288960411, s8=418019486, s9=141729623] (stddev=368195054.23, mean=531596343.67, sum=4784367093)
cpu#1: thrash_pct: [s1=78%, s2=114%, s3=97%, s4=33%, s5=68%, s6=28%, s7=65%, s8=43%, s9=22%]  (sum=548%)
cpu_util#1: last:  [s1=0.23, s2=0.20, s3=0.20, s4=0.06, s5=0.08, s6=0.03, s7=0.06, s8=0.08, s9=0.03] (stddev=0.07, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=78%, s2=114%, s3=97%, s4=33%, s5=68%, s6=28%, s7=65%, s8=43%, s9=22%]  (sum=548%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=12, s2=8, s3=7, s4=14, s5=13, s6=16, s7=13, s8=13, s9=12] (stddev=2.67, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=71%, s2=78%, s3=60%, s4=123%, s5=114%, s6=98%, s7=147%, s8=108%, s9=73%]  (sum=873%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=34, s2=33, s3=32, s4=40, s5=36, s6=37, s7=34, s8=36, s9=42] (stddev=3.09, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=110%, s2=81%, s3=50%, s4=929%, s5=908%, s6=955%, s7=1203%, s8=862%, s9=793%]  (sum=5892%)
write_bytes_per_second#1: last:  [s1=4932, s2=4491, s3=4381, s4=6876, s5=5997, s6=6698, s7=5690, s8=6146, s9=7312] (stddev=993.86, mean=5835.89, sum=52523)
write_bytes_per_second#1: thrash_pct: [s1=747%, s2=722%, s3=771%, s4=1302%, s5=1273%, s6=1314%, s7=1458%, s8=1295%, s9=1253%]  (sum=10135%)
artifacts[mma-count]: 924191edbbffb971
==========================
